{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will access the FRF TDS, list instrument types available within user specified data range, download netcdf files for the selected instrument in the data range, collate the nc files and clip them to the dates desired, and save the file to disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from siphon.catalog import TDSCatalog\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on the imports:**\n",
    "\n",
    "`os` module alows for operating system dependent functionality. Let's the script interact with underlying OS, like for handling file paths, creating/copying/moving directories, checking if a file eixts... \n",
    "\n",
    "`datetime` & `timedelta` from `datetime` module & `relativedelta` from `dateutil` module: needed these to parse dates from strings and extract dates from filenames. And determine relative time & time differences/comparisons. Needed to add `relativedelta` to get the end date/time of the query (`dataset_end`) while accounting for diff days per calendar month.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define inputs and prep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input date range:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = pd.to_datetime('2019-08-15')\n",
    "endDate   = pd.to_datetime('2019-09-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**access TDS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8m-array', 'CS01-SBE26', 'CS02-SBE26', 'CS03-SBE26', 'CS04-SBE26', 'CS05-SBE26', 'DWG-BB02', 'DWG-BB03', 'DWG-OC02', 'DWG-OC03', 'adop-2m', 'adop-3.5m', 'awac-11m', 'awac-4.5m', 'awac-5m', 'awac-6m', 'awac-8m', 'awac-jpier-11m', 'lidarHydrodynamics', 'lidarWaveGauge080', 'lidarWaveGauge090', 'lidarWaveGauge100', 'lidarWaveGauge110', 'lidarWaveGauge140', 'lidarWaveRunup', 'paros-340x-940y-top', 'paros940-200', 'paros940-250', 'sig769-300', 'sig940-300', 'sig940-400', 'sig940-600', 'waverider-17m', 'waverider-17m-1D', 'waverider-20m-1d', 'waverider-26m', 'waverider-guam', 'waverider-nags-head-nc', 'waverider-oregon-inlet-nc', 'waverider-oregoninlet-17m', 'waverider-wallops', 'waverider-wilmington-nc', 'xp100m', 'xp125m', 'xp150m', 'xp200m', 'xp340m']\n"
     ]
    }
   ],
   "source": [
    "catalog_url = 'https://chldata.erdc.dren.mil/thredds/catalog/frf/oceanography/waves/catalog.xml'\n",
    "catalog = TDSCatalog(catalog_url) # make TDSCatalog object \n",
    "print(catalog.catalog_refs)\n",
    "# instrumentsAll = list(catalog.catalog_refs.keys())\n",
    "# print(instrumentsAll)\n",
    "# print(catalog.catalog_refs[0].follow().catalog_refs.keys()) # for 8m-array\n",
    "# print(catalog.catalog_refs[1].follow().catalog_refs.keys()) # for CS01...\n",
    "# print(catalog.catalog_refs[10].follow().catalog_refs.keys()) # for aquadop 2m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TEST**\n",
    "comment everything here to run the final version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "['2019']\n",
      "['FRF-ocean_waves_8m-array_201901.nc', 'FRF-ocean_waves_8m-array_201902.nc', 'FRF-ocean_waves_8m-array_201903.nc', 'FRF-ocean_waves_8m-array_201904.nc', 'FRF-ocean_waves_8m-array_201905.nc', 'FRF-ocean_waves_8m-array_201906.nc', 'FRF-ocean_waves_8m-array_201907.nc', 'FRF-ocean_waves_8m-array_201908.nc', 'FRF-ocean_waves_8m-array_201909.nc', 'FRF-ocean_waves_8m-array_201910.nc', 'FRF-ocean_waves_8m-array_201911.nc', 'FRF-ocean_waves_8m-array_201912.nc']\n",
      "2019-01-01 00:00:00 2019-01-31 23:59:59\n",
      "2019-02-01 00:00:00 2019-02-28 23:59:59\n",
      "2019-03-01 00:00:00 2019-03-31 23:59:59\n",
      "2019-04-01 00:00:00 2019-04-30 23:59:59\n",
      "2019-05-01 00:00:00 2019-05-31 23:59:59\n",
      "2019-06-01 00:00:00 2019-06-30 23:59:59\n",
      "2019-07-01 00:00:00 2019-07-31 23:59:59\n",
      "2019-08-01 00:00:00 2019-08-31 23:59:59\n",
      "2019-09-01 00:00:00 2019-09-30 23:59:59\n",
      "2019-10-01 00:00:00 2019-10-31 23:59:59\n",
      "2019-11-01 00:00:00 2019-11-30 23:59:59\n",
      "2019-12-01 00:00:00 2019-12-31 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# test for 8-m array: \n",
    "i=0\n",
    "inst_catalog = catalog.catalog_refs[i].follow()\n",
    "# get the available years in integers \n",
    "years = [int(year) for year in inst_catalog.catalog_refs.keys() if year.isdigit()]\n",
    "print(years) \n",
    "# is it in the year range I want? \n",
    "yearsInRange = [str(year) for year in years if startDate.year <= year <= endDate.year]\n",
    "print(yearsInRange) \n",
    "\n",
    "#presets: \n",
    "has_data=False # preset as False \n",
    "instrumentsInRange = [] #pre-allocate the final list array \n",
    "\n",
    "# ADD IF: if there is yearsInRange: \n",
    "for year in yearsInRange:\n",
    "    # which data nc files are avaialbe for that instrument and year?\n",
    "    dataInYear = list(inst_catalog.catalog_refs[year].follow().datasets.keys())\n",
    "    print(dataInYear)\n",
    "\n",
    "    for ncfile in dataInYear:  \n",
    "    # get the month from the file name, from YYYYMM format before `.nc`\n",
    "        date_part = ncfile.split('_')[-1].split('.')[0]  \n",
    "        dataset_date = datetime.strptime(date_part, '%Y%m') # form the full datetime for that month \n",
    "    # \n",
    "    #   ADD TRY/EXCEPT STATEMENT HERE: error if the file doesn't match the pattern \n",
    "    # \n",
    "    # for each datast_date, define the end date depending on # of days in that month \n",
    "        dataset_start = dataset_date\n",
    "        dataset_end = dataset_start + relativedelta(months=1) - timedelta(seconds=1)\n",
    "        print(dataset_start, dataset_end)\n",
    "    # is there overlap \n",
    "        if (dataset_start <= endDate) and (dataset_end >= startDate):\n",
    "            has_data=True\n",
    "            break \n",
    "        # end of the ncfile for-loop \n",
    "    if has_data: \n",
    "        break # if has_data=T end the year in yearsInRange loop, and append to final inst list \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on `.follow`:**\n",
    "This is related to using the Siphon library for working with TDS Catalogs. TDS Catalogs are hiercharical, with nested datasets. First I'm accessing the main catalog using `TDSCatalog`, then I'm getting the \"CatalogRef\" class object from the main catalog (this is instruments list). The `follow()` method gets the catalog object nested inside each instrument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instrumentsInRange = [] \n",
    "# for i in catalog.catalog_refs:\n",
    "#     inst_catalog_ref = catalog.catalog_refs[i]\n",
    "#     inst_catalog = inst_catalog_ref() -- doen'st work \n",
    "\n",
    "#     # all avail years for instrument i: \n",
    "#     years = [int(year) for year in inst_catalog.catalog_refs.keys() if year.isdigit()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS02-SBE26\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "inst_catalog = catalog.catalog_refs[i]\n",
    "print(inst_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8m-array, 8m-array]\n"
     ]
    }
   ],
   "source": [
    "print(instrumentsInRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8m-array, 8m-array, CS02-SBE26]\n"
     ]
    }
   ],
   "source": [
    "instrumentsInRange.append(inst_catalog)\n",
    "print(instrumentsInRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruments with data in the date range:\n",
      "1. 8m-array\n",
      "2. 8m-array\n",
      "3. CS02-SBE26\n"
     ]
    }
   ],
   "source": [
    "# selected_indices = input(\"\\nEnter numbers of instrument types eg, 1,3,5: \")\n",
    "# print(selected_indices) \n",
    "\n",
    "print(\"Instruments with data in the date range:\")\n",
    "for idx, inst in enumerate(instrumentsInRange):\n",
    "    print(f\"{idx+1}. {inst}\")  # 1. 8m-array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... have list of instruments with data in the date range, now want to choose the one(s) I want and to downalod them to netcdf ...\n",
    "\n",
    "\n",
    "for now just repopulate the yearsInRange etc. Can probably save those variables from the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting instrument 8m-array...\n",
      "['FRF-ocean_waves_8m-array_201901.nc', 'FRF-ocean_waves_8m-array_201902.nc', 'FRF-ocean_waves_8m-array_201903.nc', 'FRF-ocean_waves_8m-array_201904.nc', 'FRF-ocean_waves_8m-array_201905.nc', 'FRF-ocean_waves_8m-array_201906.nc', 'FRF-ocean_waves_8m-array_201907.nc', 'FRF-ocean_waves_8m-array_201908.nc', 'FRF-ocean_waves_8m-array_201909.nc', 'FRF-ocean_waves_8m-array_201910.nc', 'FRF-ocean_waves_8m-array_201911.nc', 'FRF-ocean_waves_8m-array_201912.nc']\n",
      "2019-12-01 00:00:00 2019-12-31 23:59:59\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'access'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_start, dataset_end)\n\u001b[1;32m     33\u001b[0m dataset \u001b[38;5;241m=\u001b[39m year_catalog\u001b[38;5;241m.\u001b[39mdatasets[dataset_name]\n\u001b[0;32m---> 34\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess\u001b[49m\u001b[38;5;241m.\u001b[39murls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTPServer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m local_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(local_filename)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'access'"
     ]
    }
   ],
   "source": [
    "# choose: \n",
    "selected_instruments = ['8m-array']\n",
    "\n",
    "# set up for download \n",
    "downloadedFiles = [] \n",
    "\n",
    "for inst in selected_instruments:\n",
    "    print(f\"\\nGetting instrument {inst}...\")\n",
    "    inst_catalog = catalog.catalog_refs[inst].follow()\n",
    "    years = [int(year) for year in inst_catalog.catalog_refs.keys() if year.isdigit()]\n",
    "    yearsInRange = [str(year) for year in years if startDate.year <= year <= endDate.year]\n",
    "    \n",
    "    if not yearsInRange:\n",
    "        print(f\"There is no {inst} data available for this date range.\")\n",
    "        continue\n",
    "    for year in yearsInRange:\n",
    "        year_catalog_ref = inst_catalog.catalog_refs[year]\n",
    "        year_catalog = year_catalog_ref.follow()\n",
    "        dataInYear = list(year_catalog.datasets.keys())\n",
    "\n",
    "        print(dataInYear)\n",
    "        \n",
    "        for dataset_name in dataInYear: \n",
    "            date_part = ncfile.split('_')[-1].split('.')[0]  \n",
    "            dataset_date = datetime.strptime(date_part, '%Y%m') # form the full datetime for that month \n",
    "        # \n",
    "    #   ADD TRY/EXCEPT STATEMENT HERE: error if the file doesn't match the pattern \n",
    "    # \n",
    "    # for each datast_date, define the end date depending on # of days in that month \n",
    "            dataset_start = dataset_date\n",
    "            dataset_end = dataset_start + relativedelta(months=1) - timedelta(seconds=1)\n",
    "            print(dataset_start, dataset_end)\n",
    "            dataset = year_catalog.datasets[dataset_name]\n",
    "            url = dataset.access.urls['HTTPServer']\n",
    "            local_filename = f\"{inst}_{dataset_name}\"\n",
    "            print(local_filename)\n",
    "    # is there overlap [start_date > data > end_date]\n",
    "            # if (dataset_start <= endDate) and (dataset_end >= startDate):\n",
    "            #     # yes, then get using requests package \n",
    "            #     dataset = year_catalog.datasets[dataset_name]\n",
    "            #     url = dataset.access.urls['HTTPServer']\n",
    "            #     local_filename = f\"{inst}_{dataset_name}\"\n",
    "            #     print(local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List instruments with data in the date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumentsInRrange = []\n",
    "\n",
    "# iterate over all instruments that exist in the TDS \n",
    "for inst in catalog.catalog_refs:\n",
    "    inst_catalog = catalog.catalog_refs[inst].follow() \n",
    "    #  available years for the instrument\n",
    "    years = [int(year) for year in inst_catalog.catalog_refs.keys() if year.isdigit()]\n",
    "    yearsInRange = [str(year) for year in years if startDate.year <= year <= endDate.year]\n",
    "    \n",
    "    if yearsInRange:\n",
    "        #are there datasets within the months ?\n",
    "        has_data = False\n",
    "        for year in yearsInRange:\n",
    "            year_catalog_ref = inst_catalog.catalog_refs[year]\n",
    "            year_catalog = year_catalog_ref.follow()\n",
    "            dataInYear = list(year_catalog.datasets.keys())\n",
    "            \n",
    "            for ncfile in dataInYear:\n",
    "                try:\n",
    "                    date_part = ncfile.split('_')[-1].split('.')[0]  # Extract 'YYYYMM'\n",
    "                    dataset_date = datetime.strptime(date_part, '%Y%m')\n",
    "                except ValueError:\n",
    "                    continue  # Skipping files that don't match teh naming pattern\n",
    "                \n",
    "                #start and end dates\n",
    "                dataset_start = dataset_date\n",
    "                dataset_end = dataset_start + relativedelta(months=1) - timedelta(seconds=1)\n",
    "                \n",
    "                # overlaps?\n",
    "                if (dataset_start <= endDate) and (dataset_end >= startDate):\n",
    "                    has_data = True\n",
    "                    break \n",
    "            if has_data:\n",
    "                break  \n",
    "        if has_data:\n",
    "            instrumentsInRrange.append(inst)\n",
    "\n",
    "print(\"Instruments with data in the date range:\")\n",
    "for idx, inst in enumerate(instrumentsInRrange):\n",
    "    print(f\"{idx+1}. {inst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_instruments = ['lidarWaveGauge090']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download .nc files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadedFiles = []\n",
    "\n",
    "for inst in selected_instruments:\n",
    "    print(f\"\\nGetting instrument {inst}...\")\n",
    "    inst_catalog = catalog.catalog_refs[inst].follow()\n",
    "\n",
    "    years = [int(year) for year in inst_catalog.catalog_refs.keys() if year.isdigit()]\n",
    "    yearsInRange = [str(year) for year in years if startDate.year <= year <= endDate.year]\n",
    "    \n",
    "    if not yearsInRange:\n",
    "        print(f\"No data available for {inst} in the specified date range.\")\n",
    "        continue\n",
    "    \n",
    "    for year in yearsInRange:\n",
    "        year_catalog_ref = inst_catalog.catalog_refs[year]\n",
    "        year_catalog = year_catalog_ref.follow()\n",
    "        dataInYear = list(year_catalog.datasets.keys())\n",
    "        \n",
    "        for dataset_name in dataInYear:\n",
    "            # Extract date\n",
    "            try:\n",
    "                date_part = dataset_name.split('_')[-1].split('.')[0]  # Extract 'YYYYMM'\n",
    "                dataset_date = datetime.strptime(date_part, '%Y%m')\n",
    "            except ValueError:\n",
    "                continue  \n",
    "            \n",
    "            # start and end dates\n",
    "            dataset_start = dataset_date\n",
    "            dataset_end = dataset_start + relativedelta(months=1) - timedelta(seconds=1)\n",
    "            \n",
    "            # is there overlap [start_date > data > end_date]\n",
    "            if (dataset_start <= endDate) and (dataset_end >= startDate):\n",
    "                dataset = year_catalog.datasets[dataset_name]\n",
    "                url = dataset.access_urls['HTTPServer']\n",
    "                local_filename = f\"{inst}_{dataset_name}\"\n",
    "                \n",
    "                if not os.path.exists(local_filename):\n",
    "                    print(f\"Downloading {url} to {local_filename}\")\n",
    "                    with requests.get(url, stream=True) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(local_filename, 'wb') as f:\n",
    "                            for chunk in r.iter_content(chunk_size=8192):\n",
    "                                f.write(chunk)\n",
    "                    downloadedFiles.append(local_filename)\n",
    "                else:\n",
    "                    print(f\"{local_filename} already exists.\")\n",
    "                    downloadedFiles.append(local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate the nc files and clip to date range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = []\n",
    "for f in downloadedFiles:\n",
    "    ds = xr.open_dataset(f)\n",
    "    ds_list.append(ds)\n",
    "\n",
    "# concat along 'time' dimension\n",
    "ds_combined = xr.concat(ds_list, dim='time', data_vars='minimal', coords='minimal', compat='override', join='override')\n",
    "\n",
    "# Convert 'time' to datetime64 \n",
    "if ds_combined['time'].dtype != 'datetime64[ns]':\n",
    "    ds_combined['time'] = pd.to_datetime(ds_combined['time'].values)\n",
    "    \n",
    "# clip to range\n",
    "ds_clipped = ds_combined.sel(time=slice(startDate, endDate))    \n",
    "    \n",
    "# save to new netCDF file\n",
    "output_filename = f\"{inst}_collated_clipped_data.nc\"\n",
    "ds_clipped.to_netcdf(output_filename)\n",
    "print(f\"Collated/clipped data saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frf-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
